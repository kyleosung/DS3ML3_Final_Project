{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import chess\n",
    "\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4., -2., -3., -5., -6., -3., -2., -4.],\n",
      "        [-1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 4.,  2.,  3.,  5.,  6.,  3.,  2.,  4.]])\n"
     ]
    }
   ],
   "source": [
    "def fen_str_to_tensor(fen):\n",
    "    # Define a mapping from pieces to integers\n",
    "    piece_to_int = {\n",
    "        'P': 1, 'N': 2, 'B': 3, 'R': 4, 'Q': 5, 'K': 6,\n",
    "        'p': -1, 'n': -2, 'b': -3, 'r': -4, 'q': -5, 'k': -6,\n",
    "        '.': 0\n",
    "    }\n",
    "\n",
    "    # Split the FEN string into parts ## 'rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1'\n",
    "    parts = fen.split(' ')\n",
    "    ranks = parts[0].split('/') # Only process the board position (the first part)\n",
    "\n",
    "    # Convert the ranks to a list of integers\n",
    "    board = []\n",
    "    for rank in ranks:\n",
    "        for char in rank:\n",
    "            if char.isdigit():\n",
    "                # If the character is a digit, add that many zeros to the board\n",
    "                board.extend([0] * int(char))\n",
    "            else:\n",
    "                # Otherwise, add the integer representation of the piece to the board\n",
    "                board.append(piece_to_int[char])\n",
    "\n",
    "    # Convert the board to a tensor\n",
    "    board_tensor = torch.tensor(board, dtype=torch.float32).reshape(8,8)\n",
    "\n",
    "    return board_tensor\n",
    "\n",
    "\n",
    "fen = 'rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1'\n",
    "print(fen_str_to_tensor(fen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Chess_Jan_aa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EvalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EvalNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size = 6, stride = 1, padding = 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(400 + 2, 128) ## Add two for scalar inputs\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x, scalar_inputs):\n",
    "        x = x.unsqueeze(1)\n",
    "        # print(x.shape)\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = self.flatten(x)\n",
    "        # print(f\"x shape: {x.shape}, scalar_input shape: {scalar_inputs.shape}\")\n",
    "        x = torch.cat((x, scalar_inputs), dim=1)\n",
    "        # print(x.shape)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' NO BATCHING\\ndef train(model, dataset, criterion, optimizer, num_epochs, num_samples_per_epoch=100):\\n    print(\\'Begin Training!\\')\\n    model.train()  # Set the model to training mode\\n    for epoch in range(num_epochs):\\n        train_indices = torch.randperm(len(dataset))[:num_samples_per_epoch]\\n        val_indices = torch.randperm(len(dataset))[-num_samples_per_epoch:]\\n        train_running_loss = 0.0\\n        val_running_loss = 0.0\\n        try:\\n            ## TRAINING PHASE\\n            for idx in train_indices:\\n                print(idx.item())\\n                data = dataset[idx.item()] # grab int from one element tensor\\n\\n                # Feature Variables\\n                fen           = fen_str_to_tensor( data[\\'board\\'] ).to(device)\\n                white_active  = torch.tensor( data[\\'white_active\\'] ).to(device).unsqueeze(0)\\n                is_check      = torch.tensor( data[\\'is_check\\'] ).to(device).unsqueeze(0)\\n                scalar_inputs = torch.cat( (white_active, is_check), dim = 0 ).T\\n\\n                # Predictor Variables\\n                cp = torch.tensor( data[\\'cp\\'] ).to(device)\\n\\n                # Zero the parameter gradients\\n                optimizer.zero_grad()\\n\\n                # Forward pass\\n                train_outputs = model(fen, scalar_inputs)\\n                train_batch_loss = criterion(train_outputs, cp)\\n\\n                # Backward pass and optimization\\n                train_batch_loss.backward()\\n                optimizer.step()\\n\\n                train_running_loss += train_batch_loss.item()\\n            \\n            ## VALIDATION PHASE\\n            model.eval()  # Set the model to evaluation mode\\n        \\n            with torch.no_grad():\\n                for idx in val_indices:\\n                    # Extract the inputs and labels from the validation data\\n                    \\n                    val_data = dataset[idx.item()] # grab int from one element tensor\\n\\n                    # Feature Variables\\n                    fen           = fen_str_to_tensor( val_data[\\'board\\'] ).to(device)\\n                    white_active  = torch.tensor( val_data[\\'white_active\\'] ).to(device).unsqueeze(0)\\n                    is_check      = torch.tensor( val_data[\\'is_check\\'] ).to(device).unsqueeze(0)\\n                    scalar_inputs = torch.cat( (white_active, is_check), dim = 0 ).T\\n\\n                    # Predictor Variables\\n                    cp = torch.tensor( val_data[\\'cp\\'] ).to(device)\\n                    \\n                    # Forward pass\\n                    val_outputs = model(fen, scalar_inputs)\\n                    val_batch_loss = criterion(val_outputs, cp)\\n                    \\n                    val_loss += val_batch_loss.item()\\n\\n        except KeyboardInterrupt:\\n            print(\"Manual Stop: Finished Training Early!\")\\n            break\\n        \\n        print(f\\'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_running_loss/len(train_data_loader)}, Validation Loss: {val_running_loss/len(val_data_loader)}\\')\\n\\n    print(\\'Finished Training!\\')\\n\\n    torch.save(model, \\'models/autosave.pth\\')\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' NO BATCHING\n",
    "def train(model, dataset, criterion, optimizer, num_epochs, num_samples_per_epoch=100):\n",
    "    print('Begin Training!')\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        train_indices = torch.randperm(len(dataset))[:num_samples_per_epoch]\n",
    "        val_indices = torch.randperm(len(dataset))[-num_samples_per_epoch:]\n",
    "        train_running_loss = 0.0\n",
    "        val_running_loss = 0.0\n",
    "        try:\n",
    "            ## TRAINING PHASE\n",
    "            for idx in train_indices:\n",
    "                print(idx.item())\n",
    "                data = dataset[idx.item()] # grab int from one element tensor\n",
    "\n",
    "                # Feature Variables\n",
    "                fen           = fen_str_to_tensor( data['board'] ).to(device)\n",
    "                white_active  = torch.tensor( data['white_active'] ).to(device).unsqueeze(0)\n",
    "                is_check      = torch.tensor( data['is_check'] ).to(device).unsqueeze(0)\n",
    "                scalar_inputs = torch.cat( (white_active, is_check), dim = 0 ).T\n",
    "\n",
    "                # Predictor Variables\n",
    "                cp = torch.tensor( data['cp'] ).to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                train_outputs = model(fen, scalar_inputs)\n",
    "                train_batch_loss = criterion(train_outputs, cp)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                train_batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_running_loss += train_batch_loss.item()\n",
    "            \n",
    "            ## VALIDATION PHASE\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                for idx in val_indices:\n",
    "                    # Extract the inputs and labels from the validation data\n",
    "                    \n",
    "                    val_data = dataset[idx.item()] # grab int from one element tensor\n",
    "\n",
    "                    # Feature Variables\n",
    "                    fen           = fen_str_to_tensor( val_data['board'] ).to(device)\n",
    "                    white_active  = torch.tensor( val_data['white_active'] ).to(device).unsqueeze(0)\n",
    "                    is_check      = torch.tensor( val_data['is_check'] ).to(device).unsqueeze(0)\n",
    "                    scalar_inputs = torch.cat( (white_active, is_check), dim = 0 ).T\n",
    "\n",
    "                    # Predictor Variables\n",
    "                    cp = torch.tensor( val_data['cp'] ).to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    val_outputs = model(fen, scalar_inputs)\n",
    "                    val_batch_loss = criterion(val_outputs, cp)\n",
    "                    \n",
    "                    val_loss += val_batch_loss.item()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Manual Stop: Finished Training Early!\")\n",
    "            break\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_running_loss/len(train_data_loader)}, Validation Loss: {val_running_loss/len(val_data_loader)}')\n",
    "\n",
    "    print('Finished Training!')\n",
    "\n",
    "    torch.save(model, 'models/autosave.pth')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, train_data_loader, val_data_loader, criterion, optimizer, num_epochs):\n",
    "    print('Begin Training!')\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        train_running_loss = 0.0\n",
    "        val_running_loss = 0.0\n",
    "        try:\n",
    "            ## TRAINING PHASE\n",
    "            for i, data in enumerate(train_data_loader):\n",
    "\n",
    "                # Feature Variables\n",
    "                fen           = data['fen'].to(device)\n",
    "                white_active  = data['white_active'].to(device).unsqueeze(0)\n",
    "                is_check      = data['is_check'].to(device).unsqueeze(0)\n",
    "                scalar_inputs = torch.cat( (white_active, is_check), dim = 0 ).T\n",
    "\n",
    "                # Predictor Variables\n",
    "                cp = (data['cp'].to(device)).unsqueeze(1)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                train_outputs = model(fen, scalar_inputs)\n",
    "\n",
    "                # print(train_outputs.shape, cp.shape)\n",
    "\n",
    "                # print(torch.isnan(train_outputs).sum(), torch.isnan(cp).sum())\n",
    "\n",
    "                train_batch_loss = criterion(train_outputs, cp)\n",
    "\n",
    "                # print(train_batch_loss)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                train_batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_running_loss += train_batch_loss.item()\n",
    "            \n",
    "            ## VALIDATION PHASE\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                for i, val_data in enumerate(val_data_loader):\n",
    "                    # Extract the inputs and labels from the validation data\n",
    "                    \n",
    "                    # Feature Variables\n",
    "                    fen           = val_data['fen'].to(device)\n",
    "                    white_active  = val_data['white_active'].to(device).unsqueeze(0)\n",
    "                    is_check      = val_data['is_check'].to(device).unsqueeze(0)\n",
    "                    scalar_inputs = torch.cat( (white_active, is_check), dim = 0 ).T\n",
    "\n",
    "                    # Predictor Variables\n",
    "                    cp = (val_data['cp'].to(device)).unsqueeze(1)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    val_outputs = model(fen, scalar_inputs)\n",
    "                    val_batch_loss = criterion(val_outputs, cp)\n",
    "                    \n",
    "                    val_running_loss += val_batch_loss.item()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Manual Stop: Finished Training Early!\")\n",
    "            break\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_running_loss/len(train_data_loader)}, Validation Loss: {val_running_loss/len(val_data_loader)}')\n",
    "\n",
    "    print('Finished Training!')\n",
    "\n",
    "    torch.save(model, 'models/autosave.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader, Dataset\n",
    "\n",
    "class ChessIterableDataset(IterableDataset): # TODO! Write docstrings \n",
    "\n",
    "    def __init__(self, csv_files, chunksize):\n",
    "        self.csv_files = csv_files\n",
    "        self.chunksize = chunksize\n",
    "\n",
    "    def process_chunk(self, chunk):\n",
    "        # Process the chunk and return a list of samples\n",
    "        samples = []\n",
    "        for index, row in chunk.iterrows():\n",
    "            \n",
    "            # Feature Variables\n",
    "            board = row['board']\n",
    "            white_active = row['white_active']\n",
    "            cp = row['cp']\n",
    "            is_check = row['is_check'] # add to predictions\n",
    "\n",
    "            # Convert data to tensors\n",
    "            board_tensor = fen_str_to_tensor(board)\n",
    "            white_active = torch.tensor(white_active, dtype=torch.float32)\n",
    "            cp = torch.tensor(cp, dtype=torch.float32)\n",
    "            is_check = torch.tensor(is_check, dtype=torch.float32)\n",
    "        \n",
    "            samples.append({'fen': board_tensor, 'fen_str': board, 'white_active': white_active, 'cp': cp, 'is_check': is_check})\n",
    "        return samples\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(1 for _ in self.__iter__())\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        board = self.dataframe.iloc[idx]['board']\n",
    "        white_active = self.dataframe.iloc[idx]['white_active']\n",
    "        cp = self.dataframe.iloc[idx]['cp']\n",
    "        is_check = self.dataframe.iloc[idx]['is_check']\n",
    "        \n",
    "        # Convert data to tensors\n",
    "        board_tensor = fen_str_to_tensor(board)\n",
    "        white_active = torch.tensor(white_active, dtype=torch.float32)\n",
    "        cp = torch.tensor(cp, dtype=torch.float32)\n",
    "\n",
    "        return {'fen': board_tensor, 'fen_str': board, 'white_active': white_active, 'cp': cp, 'is_check': is_check}\n",
    "    \n",
    "\n",
    "    def __iter__(self):\n",
    "        for idx, csv_file in enumerate(self.csv_files):\n",
    "            chunk_iter = pd.read_csv(csv_file, chunksize=self.chunksize)#, dtype={15:str, 33:str})#, engine='pyarrow')\n",
    "            \n",
    "            if idx % 10 == 0:\n",
    "                # print(f'Read file {idx} of {len(self.csv_files)}')\n",
    "                pass\n",
    "\n",
    "            for chunk in chunk_iter:\n",
    "                \n",
    "                chunk = chunk.loc[chunk['white_elo'] >= 1350] # select a subset of players with a certain rating\n",
    "                chunk = chunk.loc[chunk['black_elo'] >= 1350]\n",
    "                chunk = chunk.dropna(subset=['cp'])\n",
    "                \n",
    "                yield from self.process_chunk(chunk) # Returns a list of yielded elements\n",
    "            \n",
    "            del chunk_iter # clean up garbage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# path = \"../Data/DataTrain\"\n",
    "\n",
    "path = \".\" ## use this for laptop\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "csv_files = glob.glob(f'{path}/*.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Training!\n",
      "Manual Stop: Finished Training Early!\n",
      "Finished Training!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[68], line 70\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataset, train_data_loader, val_data_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_running_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_data_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_running_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_data_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Training!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 70\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/autosave.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kyle\\OneDrive\\Programs\\Datasci_3ML3\\.venv\\lib\\site-packages\\torch\\serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    625\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    629\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kyle\\OneDrive\\Programs\\Datasci_3ML3\\.venv\\lib\\site-packages\\torch\\serialization.py:502\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    501\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kyle\\OneDrive\\Programs\\Datasci_3ML3\\.venv\\lib\\site-packages\\torch\\serialization.py:473\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 473\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory models does not exist."
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a dataset\n",
    "dataset = ChessIterableDataset(csv_files, chunksize = 50000)\n",
    "\n",
    "# Create a data loader\n",
    "train_data_loader = DataLoader(dataset, batch_size = 150)\n",
    "val_data_loader = DataLoader(dataset, batch_size = 150)\n",
    "\n",
    "# Create a model\n",
    "model = EvalNet()\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.L1Loss() # nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.03, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "train(model, dataset, train_data_loader, val_data_loader, criterion, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NO BATCHING\\nimport pandas as pd\\nfrom torch.utils.data import Dataset, DataLoader\\n\\nclass CustomDataset(Dataset):\\n    def __init__(self, csv_files):\\n        self.data = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\\n    \\n    def __len__(self):\\n        return len(self.data)\\n    \\n    def __getitem__(self, idx):\\n        sample = self.data.iloc[idx]\\n        return sample\\n\\n\\n# Create the dataset\\ndataset = CustomDataset(csv_files)\\n'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''NO BATCHING\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_files):\n",
    "        self.data = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        return sample\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CustomDataset(csv_files)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 2 required positional arguments: 'optimizer' and 'num_epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[192], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.003\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m      7\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: train() missing 2 required positional arguments: 'optimizer' and 'num_epochs'"
     ]
    }
   ],
   "source": [
    "model = EvalNet()\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "\n",
    "num_epochs = 10\n",
    "train(model, dataset, criterion, optimizer, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import chess_SL_E5_lib as lib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import chess\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4., -2., -3., -5., -6., -3., -2., -4.],\n",
       "        [-1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 4.,  2.,  3.,  5.,  6.,  3.,  2.,  4.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lib.fen_str_to_flat_tensor('rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [-1., -1., -1., -1., -1., -1., -1., -1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0., -2.,  0.,  0.,  0.,  0., -2.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  2.,  0.,  0.,  0.,  0.,  2.,  0.]],\n",
       "\n",
       "        [[ 0.,  0., -3.,  0.,  0., -3.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  3.,  0.,  0.,  3.,  0.,  0.]],\n",
       "\n",
       "        [[-4.,  0.,  0.,  0.,  0.,  0.,  0., -4.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 4.,  0.,  0.,  0.,  0.,  0.,  0.,  4.]],\n",
       "\n",
       "        [[ 0.,  0.,  0., -5.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  5.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.,  0., -6.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  6.,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lib.fen_str_to_3d_tensor('rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Data/DataTrain\"\n",
    "\n",
    "letters_in = 'abcd'\n",
    "letters_out = 'defg'\n",
    "\n",
    "# csv_files1 = csv_files2 = glob.glob(f'{path}/Chess_Jan_aa*')\n",
    "\n",
    "csv_files1 = glob.glob(f'{path}/Chess_Jan_a*') + glob.glob(f'{path}/Chess_Jan_b*') + glob.glob(f'{path}/Chess_Jan_c*') + glob.glob(f'{path}/Chess_Jan_d*')\n",
    "csv_files2 = glob.glob(f'{path}/Chess_Jan_d*') + glob.glob(f'{path}/Chess_Jan_e*')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models Trained\n",
    "- V1 -> Predictor: tensor [64,64]\n",
    "- V2 -> Predictor: tensor [64,64]\n",
    "- V3 -> Predictor: tensor [64,64]\n",
    "- V4 -> Predictor: tensor [64,64]\n",
    "\n",
    "- E1 -> Epochs: 200, SGD(Learning Rate: 0.030, Momentum = 0.9), Loss: L1Loss, Training: \"a*\", Validation: \"a*\" |||| Vars: FEN, white_active, is_check |||| Predictor: CP\n",
    "- E2 -> Epochs: 200, SGD(Learning Rate: 0.035, Momentum = 0.9), Loss: L1Loss, Training: \"a*\", Validation: \"b*\" |||| Vars: FEN, white_active, is_check |||| Predictor: CP\n",
    "- E3 -> Epochs: 75, Adam(Learning Rate: 0.006, Mometum = 0.9), Loss: L1Loss, Training: \"a|b*\", Validation: \"c|d*\" |||| Vars: FEN, white_active, is_check, is_capture |||| Predictor: CP\n",
    "- E3.1 -> Epochs: 50, Adam(Learning Rate: 0.006), Loss: L1Loss, Training: \"a|b|c*\", Validation: \"d|e|f*\" |||| Vars: FEN, white_active, is_check, is_capture |||| Predictor: CP\n",
    "- E3.2 -> Epochs: 40, Adam(Learning Rate: 0.006), Loss: L1Loss, Training: \"a|b|c|d*\", Validation: \"d|e|f|g*\" |||| Vars: FEN, white_active, is_check, is_capture |||| Predictor: CP\n",
    "- E4.1 -> Epochs: 25, Adam(Learning Rate: 0.006), Loss: L1Loss, Training: \"a|b|c|d*\", Validation: \"d|e|f|g*\" |||| Vars: FEN, white_active, is_capture |||| Predictor: CP\n",
    "- E5.1 -> Epochs: 25, Adam(Learning Rate: 0.006), Loss: L1Loss, Training: \"a|b|c|d*\", Validation: \"d|e*\" |||| Vars: FEN |||| Predictor: CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Training!\n",
      "Epoch 1/25, Training Loss: 2.62307, Validation Loss: 2.38325\n",
      "Epoch 2/25, Training Loss: 2.62226, Validation Loss: 2.45849\n",
      "Epoch 3/25, Training Loss: 2.45313, Validation Loss: 2.74495\n",
      "Epoch 4/25, Training Loss: 2.41371, Validation Loss: 2.29629\n",
      "Epoch 5/25, Training Loss: 2.27945, Validation Loss: 2.12539\n",
      "Epoch 6/25, Training Loss: 2.13997, Validation Loss: 2.25505\n",
      "Epoch 7/25, Training Loss: 1.98211, Validation Loss: 1.93360\n",
      "Epoch 8/25, Training Loss: 1.95049, Validation Loss: 1.93494\n",
      "Epoch 9/25, Training Loss: 1.89045, Validation Loss: 1.83648\n",
      "Epoch 10/25, Training Loss: 1.86129, Validation Loss: 2.11811\n",
      "Epoch 11/25, Training Loss: 1.87990, Validation Loss: 1.89563\n",
      "Epoch 12/25, Training Loss: 1.82716, Validation Loss: 1.79290\n",
      "Epoch 13/25, Training Loss: 1.80303, Validation Loss: 1.76785\n",
      "Epoch 14/25, Training Loss: 1.80224, Validation Loss: 1.82271\n",
      "Epoch 15/25, Training Loss: 1.78796, Validation Loss: 1.76572\n",
      "Epoch 16/25, Training Loss: 1.77601, Validation Loss: 1.74658\n",
      "Epoch 17/25, Training Loss: 1.77916, Validation Loss: 1.74291\n",
      "Epoch 18/25, Training Loss: 1.75408, Validation Loss: 1.75597\n",
      "Manual Stop: Finished Training Early!\n",
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset\n",
    "dataset1 = lib.ChessIterableDataset(csv_files1, chunksize = 50000)\n",
    "dataset2 = lib.ChessIterableDataset(csv_files2, chunksize = 50000)\n",
    "\n",
    "# Create a data loader\n",
    "train_data_loader = DataLoader(dataset1, batch_size = 25000)\n",
    "val_data_loader = DataLoader(dataset2, batch_size = 25000)\n",
    "\n",
    "# Create a model\n",
    "model = lib.EvalNet()\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.L1Loss() # nn.MSELoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.035, momentum=0.9)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.006)\n",
    "\n",
    "# Train the model\n",
    "training_loss_history, validation_loss_history = lib.train(model, train_data_loader, val_data_loader, criterion, optimizer, num_epochs=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_E5.pth')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7, 3.5))\n",
    "plt.plot(training_loss_history, label = 'Training Loss')\n",
    "plt.plot(validation_loss_history, label = 'Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "# plt.ylim(0, None)\n",
    "plt.title('Figure 1: Loss for E5-1 Model')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE TO SELF - TEST NEW STOCHASTIC PREDICTION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(training_loss_history, open('training_loss_history_E5-1.pkl', 'wb'))\n",
    "pickle.dump(validation_loss_history, open('validation_loss_history_E5-1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = torch.load('model_E5-1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = chess.Board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move = lib.predict(model_loaded, board.fen())\n",
    "print(move)\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board.push_uci(move)\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move = lib.predict(model_loaded, board.fen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move = 'e4f6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : make prediction function\n",
    "# TODO : speed up"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
